{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebf08645-5e8f-4358-b81d-44f08cca57a5",
   "metadata": {},
   "source": [
    "## Explanation of Key Sections\n",
    "\n",
    "This file contains functions aimed at checking, and then fixing errors according to the following data quality dimensions:\n",
    "\n",
    "Accuracy: Accuracy measures whether the data correctly reflects the real-world entities or events it is supposed to represent. Note that although records can be checked with rules (e.g. no-one can be an age under 0), this will not check for innaccuracies that still conform to these rules.\n",
    "\n",
    "Completeness: Completeness measures whether all required data is present. Missing or incomplete data diminishes the utility of a dataset.\n",
    "\n",
    "Uniqueness: Data should not have unnecessary duplicates.\n",
    "\n",
    "Cleanliness/Validity: Data should conform to defined formats, rules, or standards.\n",
    "\n",
    "\n",
    "\n",
    "Timeliness: Data should be available when needed and reflect the most up-to-date information.\n",
    "\n",
    "Consistency: Data should be consistent across different systems or datasets, and therefore should not conflict between systems or datasets.\n",
    "\n",
    "Notes\n",
    "Completeness: Calculates missing values per column. Will need function to return error rate percentage summary, and function to return all error rows in a df.\n",
    "\n",
    "Accuracy: Checks for invalid values based on user-defined rules. Will need function to return error rate percentage summary, and function to return all error rows in a df. \n",
    "Flags records with future dates?\n",
    "\n",
    "Relevance: Identifies columns with low variance.\n",
    "\n",
    "Cleanliness: Strips, lowercases, and standardizes string columns. needs to be done first?  Will need function to return error rate percentage summary, and function to return all error rows in a df, AND a function to actually clean the data. ALSO needs a seperate function to fill blanks with mean median etc.\n",
    "\n",
    "Uniqueness: Finds duplicate rows or entries in specified columns.\n",
    "\n",
    "\n",
    "Consistency: Verifies logical relationships between columns.\n",
    "\n",
    "Reliability: Detects statistical outliers.\n",
    "\n",
    "Integrity: Confirms foreign key relationships.\n",
    "\n",
    "Redundancy: Highlights highly correlated columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6921151e-46b4-4aa7-8b80-28c8416729fb",
   "metadata": {},
   "source": [
    "## Cleaning Functions\n",
    "The following functions perform several data cleaning steps important for a wide range of datasets.\n",
    "\n",
    "### Summary Checks\n",
    "Some provide checks that measure the count and percentage of data quality issues within the original Dataframe.\n",
    "\n",
    "### Specific Row Searches\n",
    "Some locate and display the explicit rows which have data quality issues within the original Dataframe.\n",
    "\n",
    "### Data Cleaning Functions\n",
    "Some clean and amend the data which has quality issues within the original Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79d25ce7-e1f7-42a8-8418-cef723481fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Completeness Check:\n",
      "          missing_count  missing_percentage\n",
      "Name                  1                20.0\n",
      "Age                   1                20.0\n",
      "JoinDate              1                20.0\n",
      "ID                    0                 0.0\n",
      "\n",
      "2. Accuracy Check:\n",
      "\n",
      "Invalid Age entries:\n",
      "      Name  Age    JoinDate  ID\n",
      "1      Bob -1.0  2025-01-01   2\n",
      "3  David B  NaN        None   3\n",
      "\n",
      "Invalid JoinDate entries:\n",
      "      Name  Age JoinDate  ID\n",
      "3  David B  NaN     None   3\n",
      "\n",
      "3. Accuracy Summary:\n",
      "          error_count  error_percentage\n",
      "Name              NaN               NaN\n",
      "Age               2.0              40.0\n",
      "JoinDate          1.0              20.0\n",
      "ID                NaN               NaN\n",
      "\n",
      "4. Uniqueness Check for ID column:\n",
      "      Name   Age    JoinDate  ID\n",
      "2  Charlie  30.0  2019-07-15   3\n",
      "3  David B   NaN        None   3\n",
      "\n",
      "5. Uniqueness Summary for ID column:\n",
      "  column  duplicate_count  duplicate_percentage\n",
      "0     ID                2                  40.0\n",
      "\n",
      "6. Converting Data Types:\n",
      "Name                object\n",
      "Age                float64\n",
      "JoinDate    datetime64[ns]\n",
      "ID                   int64\n",
      "dtype: object\n",
      "\n",
      "7. Cleaning Text Columns:\n",
      "      Name   Age    JoinDate  ID\n",
      "0    Alice  25.0  2021-01-01   1\n",
      "1      Bob  -1.0  2025-01-01   2\n",
      "2  Charlie  30.0  2019-07-15   3\n",
      "3  David B   NaN        None   3\n",
      "4           40.0  2018-12-10   5\n",
      "\n",
      "8. Handling Missing Values:\n",
      "      Name   Age   JoinDate  ID\n",
      "0   Alice   25.0 2021-01-01   1\n",
      "1      Bob  -1.0 2025-01-01   2\n",
      "2  Charlie  30.0 2019-07-15   3\n",
      "4  Unknown  40.0 2018-12-10   5\n",
      "\n",
      "9. Standardizing Case:\n",
      "      Name   Age    JoinDate  ID\n",
      "0    Alice  25.0  2021-01-01   1\n",
      "1      Bob  -1.0  2025-01-01   2\n",
      "2  Charlie  30.0  2019-07-15   3\n",
      "3  David B   NaN        None   3\n",
      "4           40.0  2018-12-10   5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class DataQualityToolkit:\n",
    "    \"\"\"\n",
    "    A collection of functions for performing data quality checks and cleaning operations\n",
    "    on pandas DataFrames.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_completeness(df):\n",
    "        \"\"\"\n",
    "        Checks the completeness of a DataFrame by identifying missing values.\n",
    "        \n",
    "        Parameters:\n",
    "            df (pd.DataFrame): The DataFrame to check\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Summary of missing values count and percentage by column\n",
    "        \"\"\"\n",
    "        missing_summary = df.isnull().sum().to_frame('missing_count')\n",
    "        missing_summary['missing_percentage'] = (missing_summary['missing_count'] / len(df)) * 100\n",
    "        return missing_summary\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_accuracy(df, rules):\n",
    "        \"\"\"\n",
    "        Checks data accuracy against a set of validation rules.\n",
    "        \n",
    "        Parameters:\n",
    "            df (pd.DataFrame): The DataFrame to check\n",
    "            rules (dict): Dictionary where keys are column names and values are lambda \n",
    "                          functions that return True for valid values\n",
    "                          \n",
    "        Returns:\n",
    "            dict: Dictionary of DataFrames containing invalid entries for each column\n",
    "        \"\"\"\n",
    "        issues = {}\n",
    "        for column, rule in rules.items():\n",
    "            if column in df.columns:\n",
    "                invalid_entries = ~df[column].apply(rule)\n",
    "                issues[column] = df[invalid_entries]\n",
    "        return issues\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_accuracy_summary(df, rules):\n",
    "        \"\"\"\n",
    "        Provides a summary of accuracy issues based on validation rules.\n",
    "        \n",
    "        Parameters:\n",
    "            df (pd.DataFrame): The DataFrame to check\n",
    "            rules (dict): Dictionary where keys are column names and values are lambda \n",
    "                          functions that return True for valid values\n",
    "                          \n",
    "        Returns:\n",
    "            pd.DataFrame: Summary of error counts and percentages by column\n",
    "        \"\"\"\n",
    "        summary = {'error_count': {}, 'error_percentage': {}}\n",
    "        \n",
    "        for column in df.columns:\n",
    "            if column in rules:\n",
    "                invalid_entries = ~df[column].apply(rules[column])\n",
    "                error_count = invalid_entries.sum()\n",
    "                error_percentage = (error_count / len(df)) * 100\n",
    "            else:\n",
    "                error_count = np.nan\n",
    "                error_percentage = np.nan\n",
    "            \n",
    "            summary['error_count'][column] = error_count\n",
    "            summary['error_percentage'][column] = error_percentage\n",
    "            \n",
    "        return pd.DataFrame(summary)\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_uniqueness(df, column):\n",
    "        \"\"\"\n",
    "        Identifies duplicate values in a specified column.\n",
    "        \n",
    "        Parameters:\n",
    "            df (pd.DataFrame): The DataFrame to check\n",
    "            column (str): The column to check for duplicate values\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Rows with duplicate values in the specified column\n",
    "        \"\"\"\n",
    "        duplicates = df.duplicated(subset=[column], keep=False)\n",
    "        return df[duplicates]\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_uniqueness_summary(df, column):\n",
    "        \"\"\"\n",
    "        Provides a summary of duplicate values in a specified column.\n",
    "        \n",
    "        Parameters:\n",
    "            df (pd.DataFrame): The DataFrame to check\n",
    "            column (str): The column to check for duplicate values\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Summary of duplicate counts and percentages\n",
    "        \"\"\"\n",
    "        duplicate_mask = df.duplicated(subset=[column], keep=False)\n",
    "        duplicate_count = duplicate_mask.sum()\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'column': [column],\n",
    "            'duplicate_count': [duplicate_count],\n",
    "            'duplicate_percentage': [(duplicate_count / len(df)) * 100]\n",
    "        })\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_uniqueness_multiple_columns(df, columns):\n",
    "        \"\"\"\n",
    "        Identifies duplicate values across multiple columns.\n",
    "        \n",
    "        Parameters:\n",
    "            df (pd.DataFrame): The DataFrame to check\n",
    "            columns (list): List of columns to check for duplicate values\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Rows with duplicate values across the specified columns\n",
    "        \"\"\"\n",
    "        duplicates = df.duplicated(subset=columns, keep=False)\n",
    "        return df[duplicates]\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_text_columns(df, columns=None):\n",
    "        \"\"\"\n",
    "        Cleans text columns by standardizing format and removing extra whitespace.\n",
    "        \n",
    "        Parameters:\n",
    "            df (pd.DataFrame): The DataFrame to clean\n",
    "            columns (list, optional): List of column names to clean. If None, \n",
    "                                      all object/string columns are cleaned\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with cleaned text columns\n",
    "        \"\"\"\n",
    "        # Create a copy to avoid modifying the original DataFrame\n",
    "        df_cleaned = df.copy()\n",
    "        \n",
    "        # If columns not specified, select all object/string columns\n",
    "        if columns is None:\n",
    "            columns = df_cleaned.select_dtypes(include=['object', 'string']).columns\n",
    "        \n",
    "        for col in columns:\n",
    "            if col in df_cleaned.select_dtypes(include=['object', 'string']).columns:\n",
    "                # Handle NaN values\n",
    "                df_cleaned[col] = df_cleaned[col].fillna('')\n",
    "                # Clean strings\n",
    "                df_cleaned[col] = df_cleaned[col].astype(str).str.strip().str.replace(r'\\s+', ' ', regex=True)\n",
    "                \n",
    "        return df_cleaned\n",
    "    \n",
    "    @staticmethod\n",
    "    def standardize_case(df, columns, case='lower'):\n",
    "        \"\"\"\n",
    "        Standardizes text case in specified columns.\n",
    "        \n",
    "        Parameters:\n",
    "            df (pd.DataFrame): The DataFrame to process\n",
    "            columns (list): List of column names to standardize\n",
    "            case (str): Case to convert to ('lower', 'upper', or 'title')\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with standardized case in specified columns\n",
    "        \"\"\"\n",
    "        df_standardized = df.copy()\n",
    "        \n",
    "        for col in columns:\n",
    "            if col in df_standardized.columns:\n",
    "                if case == 'lower':\n",
    "                    df_standardized[col] = df_standardized[col].astype(str).str.lower()\n",
    "                elif case == 'upper':\n",
    "                    df_standardized[col] = df_standardized[col].astype(str).str.upper()\n",
    "                elif case == 'title':\n",
    "                    df_standardized[col] = df_standardized[col].astype(str).str.title()\n",
    "                    \n",
    "        return df_standardized\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_outliers(df, column, method='iqr', threshold=1.5):\n",
    "        \"\"\"\n",
    "        Removes outliers from a numeric column.\n",
    "        \n",
    "        Parameters:\n",
    "            df (pd.DataFrame): The DataFrame to process\n",
    "            column (str): The column to check for outliers\n",
    "            method (str): Method to use ('iqr' or 'zscore')\n",
    "            threshold (float): Threshold for outlier detection (default: 1.5 for IQR, 3 for z-score)\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with outliers removed\n",
    "        \"\"\"\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        if method == 'iqr':\n",
    "            Q1 = df_clean[column].quantile(0.25)\n",
    "            Q3 = df_clean[column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - threshold * IQR\n",
    "            upper_bound = Q3 + threshold * IQR\n",
    "            df_clean = df_clean[(df_clean[column] >= lower_bound) & (df_clean[column] <= upper_bound)]\n",
    "            \n",
    "        elif method == 'zscore':\n",
    "            z_scores = np.abs((df_clean[column] - df_clean[column].mean()) / df_clean[column].std())\n",
    "            df_clean = df_clean[z_scores < threshold]\n",
    "            \n",
    "        return df_clean\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_datatypes(df, type_mapping):\n",
    "        \"\"\"\n",
    "        Converts columns to specified data types.\n",
    "        \n",
    "        Parameters:\n",
    "            df (pd.DataFrame): The DataFrame to process\n",
    "            type_mapping (dict): Dictionary mapping column names to desired data types\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with converted data types\n",
    "        \"\"\"\n",
    "        df_converted = df.copy()\n",
    "        \n",
    "        for column, dtype in type_mapping.items():\n",
    "            if column in df_converted.columns:\n",
    "                try:\n",
    "                    if dtype == 'datetime':\n",
    "                        df_converted[column] = pd.to_datetime(df_converted[column], errors='coerce')\n",
    "                    else:\n",
    "                        df_converted[column] = df_converted[column].astype(dtype)\n",
    "                except:\n",
    "                    print(f\"Failed to convert column '{column}' to {dtype}\")\n",
    "                    \n",
    "        return df_converted\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_missing_values(df, strategy_mapping):\n",
    "        \"\"\"\n",
    "        Handles missing values according to specified strategies.\n",
    "        \n",
    "        Parameters:\n",
    "            df (pd.DataFrame): The DataFrame to process\n",
    "            strategy_mapping (dict): Dictionary mapping column names to strategies\n",
    "                                    ('drop', 'mean', 'median', 'mode', or a fill value)\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with handled missing values\n",
    "        \"\"\"\n",
    "        df_handled = df.copy()\n",
    "        \n",
    "        for column, strategy in strategy_mapping.items():\n",
    "            if column in df_handled.columns:\n",
    "                if strategy == 'drop':\n",
    "                    df_handled = df_handled.dropna(subset=[column])\n",
    "                elif strategy == 'mean':\n",
    "                    df_handled[column] = df_handled[column].fillna(df_handled[column].mean())\n",
    "                elif strategy == 'median':\n",
    "                    df_handled[column] = df_handled[column].fillna(df_handled[column].median())\n",
    "                elif strategy == 'mode':\n",
    "                    df_handled[column] = df_handled[column].fillna(df_handled[column].mode()[0])\n",
    "                else:\n",
    "                    df_handled[column] = df_handled[column].fillna(strategy)\n",
    "                    \n",
    "        return df_handled\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample DataFrame\n",
    "    data = {\n",
    "        'Name': ['Alice ', ' Bob', 'Charlie', 'David B', None],\n",
    "        'Age': [25, -1, 30, None, 40],\n",
    "        'JoinDate': ['2021-01-01', '2025-01-01', '2019-07-15', None, '2018-12-10'],\n",
    "        'ID': [1, 2, 3, 3, 5]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create toolkit instance\n",
    "    toolkit = DataQualityToolkit()\n",
    "    \n",
    "    # 1. Check completeness\n",
    "    print(\"1. Completeness Check:\")\n",
    "    completeness_summary = toolkit.check_completeness(df)\n",
    "    print(completeness_summary)\n",
    "    \n",
    "    # 2. Check accuracy\n",
    "    print(\"\\n2. Accuracy Check:\")\n",
    "    rules = {\n",
    "        'Age': lambda x: x >= 0 if pd.notnull(x) else False,\n",
    "        'JoinDate': lambda x: pd.to_datetime(x, errors='coerce') <= pd.Timestamp.now() if pd.notnull(x) else False\n",
    "    }\n",
    "    accuracy_issues = toolkit.check_accuracy(df, rules)\n",
    "    for column, issues in accuracy_issues.items():\n",
    "        print(f\"\\nInvalid {column} entries:\")\n",
    "        print(issues)\n",
    "    \n",
    "    # 3. Check accuracy summary\n",
    "    print(\"\\n3. Accuracy Summary:\")\n",
    "    accuracy_summary = toolkit.check_accuracy_summary(df, rules)\n",
    "    print(accuracy_summary)\n",
    "    \n",
    "    # 4. Check uniqueness\n",
    "    print(\"\\n4. Uniqueness Check for ID column:\")\n",
    "    duplicates = toolkit.check_uniqueness(df, 'ID')\n",
    "    print(duplicates)\n",
    "    \n",
    "    # 5. Get uniqueness summary\n",
    "    print(\"\\n5. Uniqueness Summary for ID column:\")\n",
    "    uniqueness_summary = toolkit.check_uniqueness_summary(df, 'ID')\n",
    "    print(uniqueness_summary)\n",
    "    \n",
    "    # 6. Convert data types\n",
    "    print(\"\\n6. Converting Data Types:\")\n",
    "    type_mapping = {'Age': 'float64', 'JoinDate': 'datetime'}\n",
    "    df_converted = toolkit.convert_datatypes(df, type_mapping)\n",
    "    print(df_converted.dtypes)\n",
    "    \n",
    "    # 7. Clean text columns\n",
    "    print(\"\\n7. Cleaning Text Columns:\")\n",
    "    df_cleaned = toolkit.clean_text_columns(df, ['Name'])\n",
    "    print(df_cleaned)\n",
    "    \n",
    "    # 8. Handle missing values\n",
    "    print(\"\\n8. Handling Missing Values:\")\n",
    "    strategy_mapping = {'Age': 'mean', 'Name': 'Unknown', 'JoinDate': 'drop'}\n",
    "    df_handled = toolkit.handle_missing_values(df_converted, strategy_mapping)\n",
    "    print(df_handled)\n",
    "    \n",
    "    # 9. Standardize case\n",
    "    print(\"\\n9. Standardizing Case:\")\n",
    "    df_standardized = toolkit.standardize_case(df_cleaned, ['Name'], case='title')\n",
    "    print(df_standardized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
